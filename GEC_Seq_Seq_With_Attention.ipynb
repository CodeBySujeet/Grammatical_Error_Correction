{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","authorship_tag":"ABX9TyN9Y4i63Ap8ekcWL2Pghzld"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Mount Google Drive to access the dataset\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6kpnivnBE229","executionInfo":{"status":"ok","timestamp":1701361181558,"user_tz":-330,"elapsed":4680,"user":{"displayName":"Tejas Amritkar","userId":"17689469195552350370"}},"outputId":"a15a043c-fc35-493d-ae9b-d601f6a0e573"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# import the required libraries\n","from __future__ import unicode_literals, print_function, division\n","from io import open\n","import unicodedata\n","import re\n","import random\n","import csv\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","from nltk.translate import chrf_score,bleu_score\n","\n","import numpy as np\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"e7WM7NwyLRU_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We’ll need a unique index per word to use as the inputs and targets of the networks\n","#  later. To keep track of all this we will use a helper class called Lang which has\n","#   word → index (word2index) and index → word (index2word) dictionaries, as well as\n","#    a count of each word word2count which will be used to replace rare words later.\n","SOS_token = 0\n","EOS_token = 1\n","\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n","        self.n_words = 2  # Count SOS and EOS\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1"],"metadata":{"id":"AuPvPVQzJmwJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Turn a Unicode string to plain ASCII, thanks to\n","# The files are all in Unicode, to simplify we will turn Unicode characters to ASCII,\n","#  make everything lowercase, and trim most punctuation.\n","\n","\n","# https://stackoverflow.com/a/518232/2809427\n","def unicodeToAscii(s):\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn'\n","    )\n","\n","# Lowercase, trim, and remove non-letter characters\n","def normalizeString(s):\n","    s = unicodeToAscii(s.lower().strip())\n","    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n","    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n","    return s.strip()"],"metadata":{"id":"vqujVy-saTul"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To read the data file we will split the file into lines, and then split lines into pairs.\n","#  The files are all Incorrect → Correct\n","\n","def readLangs(lang1, lang2, reverse=False):\n","    print(\"Reading lines...\")\n","\n","    # Specify the path to your CSV file\n","    csv_file_path = '/content/drive/MyDrive/EE782_Grammer_Checker_Project/small_dataframe.csv'\n","\n","    # Read the CSV file and split into lines\n","    with open(csv_file_path, 'r', encoding='utf-8') as csvfile:\n","        reader = csv.reader(csvfile)\n","        lines = [row for row in reader]\n","\n","    # Normalize the strings\n","    pairs = [[normalizeString(row[0]), normalizeString(row[1])] for row in lines]\n","    input_lang = Lang(lang1)\n","    output_lang = Lang(lang2)\n","\n","    return input_lang, output_lang, pairs"],"metadata":{"id":"B1sf_crtJqFw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#  maximum length is 20 words (that includes ending punctuation) and we’re filtering\n","#  to sentences that translate to the form “I am” or “He is” etc. (accounting for\n","# apostrophes replaced earlier).\n","MAX_LENGTH = 20\n","\n","eng_prefixes = (\n","    \"i am \", \"i m \",\n","    \"he is\", \"he s \",\n","    \"she is\", \"she s \",\n","    \"you are\", \"you re \",\n","    \"we are\", \"we re \",\n","    \"they are\", \"they re \"\n",")\n","\n","def filterPair(p):\n","    return len(p[0].split(' ')) < MAX_LENGTH and \\\n","        len(p[1].split(' ')) < MAX_LENGTH #and \\\n","        #p[1].startswith(eng_prefixes)\n","\n","\n","def filterPairs(pairs):\n","    return [pair for pair in pairs if filterPair(pair)]"],"metadata":{"id":"_3m36kIOJupt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The full process for preparing the data is:\n","# Normalize text, filter by length and content\n","# Make word lists from sentences in pairs\n","\n","def prepareData(lang1, lang2, reverse=False):\n","    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n","    print(\"Read %s sentence pairs\" % len(pairs))\n","    pairs = filterPairs(pairs)\n","    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n","    print(\"Counting words...\")\n","    for pair in pairs:\n","        input_lang.addSentence(pair[0])\n","        output_lang.addSentence(pair[1])\n","    print(\"Counted words:\")\n","    print(input_lang.name, input_lang.n_words)\n","    print(output_lang.name, output_lang.n_words)\n","    return input_lang, output_lang, pairs\n","\n","input_lang, output_lang, pairs = prepareData('incorrect', 'coorect', True)\n","print(random.choice(pairs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nsEneTj_JyJS","executionInfo":{"status":"ok","timestamp":1701361182442,"user_tz":-330,"elapsed":913,"user":{"displayName":"Tejas Amritkar","userId":"17689469195552350370"}},"outputId":"e4b78684-6555-49f6-ae2e-6cac37b8a18b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading lines...\n","Read 10001 sentence pairs\n","Trimmed to 5306 sentence pairs\n","Counting words...\n","Counted words:\n","incorrect 13551\n","coorect 12466\n","['rd m grissom singled to lefted', 'rd m grissom singled to left']\n"]}]},{"cell_type":"code","source":["# The Encoder\n","# The encoder of a seq2seq network is a RNN that outputs some value for\n","#  every word from the input sentence. For every input word the encoder\n","#  outputs a vector and a hidden state, and uses the hidden state for\n","#   the next input word.\n","\n","class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n","        self.dropout = nn.Dropout(dropout_p)\n","\n","    def forward(self, input):\n","        embedded = self.dropout(self.embedding(input))\n","        output, hidden = self.gru(embedded)\n","        return output, hidden"],"metadata":{"id":"5_ztmCPOLUFM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The Decoder\n","# The decoder is another RNN that takes the encoder output vector(s)\n","#  and outputs a sequence of words to create the translation.\n","\n","# class DecoderRNN(nn.Module):\n","#     def __init__(self, hidden_size, output_size):\n","#         super(DecoderRNN, self).__init__()\n","#         self.embedding = nn.Embedding(output_size, hidden_size)\n","#         self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n","#         self.out = nn.Linear(hidden_size, output_size)\n","\n","#     def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n","#         batch_size = encoder_outputs.size(0)\n","#         decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n","#         decoder_hidden = encoder_hidden\n","#         decoder_outputs = []\n","\n","#         for i in range(MAX_LENGTH):\n","#             decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n","#             decoder_outputs.append(decoder_output)\n","\n","#             if target_tensor is not None:\n","#                 # Teacher forcing: Feed the target as the next input\n","#                 decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n","#             else:\n","#                 # Without teacher forcing: use its own predictions as the next input\n","#                 _, topi = decoder_output.topk(1)\n","#                 decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n","\n","#         decoder_outputs = torch.cat(decoder_outputs, dim=1)\n","#         decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n","#         return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n","\n","#     def forward_step(self, input, hidden):\n","#         output = self.embedding(input)\n","#         output = F.relu(output)\n","#         output, hidden = self.gru(output, hidden)\n","#         output = self.out(output)\n","#         return output, hidden"],"metadata":{"id":"CkPhi8amLyJR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Bahdanau attention, also known as additive attention, is a commonly used\n","#  attention mechanism in sequence-to-sequence models, particularly in\n","#  neural machine translation tasks.\n","\n","class CustomAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(CustomAttention, self).__init__()\n","        self.Wa = nn.Linear(hidden_size, hidden_size)\n","        self.Ua = nn.Linear(hidden_size, hidden_size)\n","        self.Va = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, query, keys):\n","        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n","        scores = scores.squeeze(2).unsqueeze(1)\n","\n","        weights = F.softmax(scores, dim=-1)\n","        context = torch.bmm(weights, keys)\n","\n","        return context, weights\n","\n","# Attention allows the decoder network to “focus” on a different part of the encoder’s\n","#  outputs for every step of the decoder’s own outputs.\n","\n","class AttnDecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n","        super(AttnDecoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.attention = CustomAttention(hidden_size)\n","        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.dropout = nn.Dropout(dropout_p)\n","\n","    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n","        batch_size = encoder_outputs.size(0)\n","        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n","        decoder_hidden = encoder_hidden\n","        decoder_outputs = []\n","        attentions = []\n","\n","        for i in range(MAX_LENGTH):\n","            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n","                decoder_input, decoder_hidden, encoder_outputs\n","            )\n","            decoder_outputs.append(decoder_output)\n","            attentions.append(attn_weights)\n","\n","            if target_tensor is not None:\n","                # Teacher forcing: Feed the target as the next input\n","                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n","            else:\n","                # Without teacher forcing: use its own predictions as the next input\n","                _, topi = decoder_output.topk(1)\n","                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n","\n","        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n","        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n","        attentions = torch.cat(attentions, dim=1)\n","\n","        return decoder_outputs, decoder_hidden, attentions\n","\n","\n","    def forward_step(self, input, hidden, encoder_outputs):\n","        embedded =  self.dropout(self.embedding(input))\n","\n","        query = hidden.permute(1, 0, 2)\n","        context, attn_weights = self.attention(query, encoder_outputs)\n","        input_gru = torch.cat((embedded, context), dim=2)\n","\n","        output, hidden = self.gru(input_gru, hidden)\n","        output = self.out(output)\n","\n","        return output, hidden, attn_weights"],"metadata":{"id":"7NMyj6bXL0MJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To train, for each pair we will need an input tensor\n","#  (indexes of the words in the input sentence) and target tensor\n","#   (indexes of the words in the target sentence). While creating\n","#   these vectors we will append the EOS token to both sequences.\n","\n","def indexesFromSentence(lang, sentence):\n","    return [lang.word2index[word] for word in sentence.split(' ')]\n","\n","def tensorFromSentence(lang, sentence):\n","    indexes = indexesFromSentence(lang, sentence)\n","    indexes.append(EOS_token)\n","    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n","\n","def tensorsFromPair(pair):\n","    input_tensor = tensorFromSentence(input_lang, pair[0])\n","    target_tensor = tensorFromSentence(output_lang, pair[1])\n","    return (input_tensor, target_tensor)\n","\n","def get_dataloader(batch_size, input_lang, output_lang, pairs):\n","    n = len(pairs)\n","    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n","    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n","\n","    for idx, (inp, tgt) in enumerate(pairs):\n","        inp_ids = indexesFromSentence(input_lang, inp)\n","        tgt_ids = indexesFromSentence(output_lang, tgt)\n","        inp_ids.append(EOS_token)\n","        tgt_ids.append(EOS_token)\n","        input_ids[idx, :len(inp_ids)] = inp_ids\n","        target_ids[idx, :len(tgt_ids)] = tgt_ids\n","\n","    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n","                               torch.LongTensor(target_ids).to(device))\n","\n","    train_sampler = RandomSampler(train_data)\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","    return input_lang, output_lang, train_dataloader"],"metadata":{"id":"kq0MMv4rKT-J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To train we run the input sentence through the encoder, and keep track of every\n","# output and the latest hidden state. Then the decoder is given the <SOS> token as\n","# its first input, and the last hidden state of the encoder as its first hidden state.\n","\n","def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n","          decoder_optimizer, criterion):\n","\n","    total_loss = 0\n","    for data in dataloader:\n","        input_tensor, target_tensor = data\n","        input_tensor = input_tensor.to(device)\n","        target_tensor = target_tensor.to(device)\n","\n","        encoder_optimizer.zero_grad()\n","        decoder_optimizer.zero_grad()\n","\n","        encoder_outputs, encoder_hidden = encoder(input_tensor)\n","        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n","\n","        loss = criterion(\n","            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n","            target_tensor.view(-1)\n","        )\n","        loss.backward()\n","\n","        encoder_optimizer.step()\n","        decoder_optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    return total_loss / len(dataloader)"],"metadata":{"id":"1VTq9PjgL4Xd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This is a helper function to print time elapsed and estimated time\n","#  remaining given the current time and progress %.\n","import time\n","import math\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"],"metadata":{"id":"7njRBnu0MoTP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The whole training process is desinged in followign way:\n","# Start a timer\n","# Initialize optimizers and criterion\n","# Create set of training pairs\n","# Start empty losses array for plotting\n","\n","def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n","               print_every=100, plot_every=100):\n","    start = time.time()\n","    plot_losses = []\n","    print_loss_total = 0  # Reset every print_every\n","    plot_loss_total = 0  # Reset every plot_every\n","\n","    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n","    criterion = nn.NLLLoss()\n","\n","    for epoch in range(1, n_epochs + 1):\n","        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n","        print_loss_total += loss\n","        plot_loss_total += loss\n","\n","        if epoch % print_every == 0:\n","            print_loss_avg = print_loss_total / print_every\n","            print_loss_total = 0\n","            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n","                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n","\n","        if epoch % plot_every == 0:\n","            plot_loss_avg = plot_loss_total / plot_every\n","            plot_losses.append(plot_loss_avg)\n","            plot_loss_total = 0\n","\n","    showPlot(plot_losses)"],"metadata":{"id":"Ewvb0GRtMukV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lotting is done with matplotlib, using the array of\n","# loss values plot_losses saved while training.\n","\n","import matplotlib.pyplot as plt\n","plt.switch_backend('agg')\n","import matplotlib.ticker as ticker\n","import numpy as np\n","\n","def showPlot(points):\n","    plt.figure()\n","    fig, ax = plt.subplots()\n","    # this locator puts ticks at regular intervals\n","    loc = ticker.MultipleLocator(base=0.2)\n","    ax.yaxis.set_major_locator(loc)\n","    plt.plot(points)"],"metadata":{"id":"YK5FU4ObQaf_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluation is mostly the same as training, but there are no targets so we simply\n","#  feed the decoder’s predictions back to itself for each step.\n","\n","def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n","    with torch.no_grad():\n","        input_tensor = tensorFromSentence(input_lang, sentence)\n","\n","        encoder_outputs, encoder_hidden = encoder(input_tensor)\n","        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n","\n","        _, topi = decoder_outputs.topk(1)\n","        decoded_ids = topi.squeeze()\n","\n","        decoded_words = []\n","        for idx in decoded_ids:\n","            if idx.item() == EOS_token:\n","                decoded_words.append('<EOS>')\n","                break\n","            decoded_words.append(output_lang.index2word[idx.item()])\n","    return decoded_words, decoder_attn"],"metadata":{"id":"HDaxrCA-Kg4i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We can evaluate random sentences from the training set and print out the input,\n","#  target, and output to make some subjective quality judgements:\n","# we also find the chrf_score here\n","\n","def evaluateRandomly(encoder, decoder, test_pairs, n=30):\n","    score = 0\n","    for i in range(n):\n","        pair = random.choice(test_pairs)\n","        src, trg = pair\n","        print('>', src)\n","        print('=', trg)\n","        output_words, _ = evaluate(encoder, decoder, src, input_lang, output_lang)\n","        prediction = ' '.join(output_words)\n","        score += chrf_score.sentence_chrf(prediction, trg)\n","        print('<', prediction)\n","        print('')\n","    return score"],"metadata":{"id":"e3frljgzKjq9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hidden_size = 128\n","batch_size = 32\n","input_lang, output_lang, pairs = prepareData('correct', 'incorrect', True)\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","train_pairs, test_pairs = train_test_split(pairs, test_size=0.01, random_state=42)\n","\n","input_lang, output_lang, train_dataloader = get_dataloader(batch_size, input_lang, output_lang, train_pairs)\n","\n","encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n","decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r6rP_3H9Kldw","executionInfo":{"status":"ok","timestamp":1701361183143,"user_tz":-330,"elapsed":782,"user":{"displayName":"Tejas Amritkar","userId":"17689469195552350370"}},"outputId":"dc11d4e6-1577-4884-8548-025d6223676f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading lines...\n","Read 10001 sentence pairs\n","Trimmed to 5306 sentence pairs\n","Counting words...\n","Counted words:\n","correct 13551\n","incorrect 12466\n"]}]},{"cell_type":"code","source":["df = pd.DataFrame(train_pairs)"],"metadata":{"id":"mYKpy6_P-l5N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"ebrXMHCo-aW3","executionInfo":{"status":"ok","timestamp":1701361183143,"user_tz":-330,"elapsed":24,"user":{"displayName":"Tejas Amritkar","userId":"17689469195552350370"}},"outputId":"64fabcaf-7589-467b-b24c-9c0d6ea92e5f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                      0  \\\n","0     the photo s of juha flinkman miia laine and sa...   \n","1     ashburnham close is located within celebrated ...   \n","2     sorry to be less specific on here figured most...   \n","3     s how to decorate and bathroom sets safe home ...   \n","4     osa scouting updated rating potential contact ...   \n","...                                                 ...   \n","5247  prediction about its upcoming tournament perfo...   \n","5248  tuition december tuition will be send home wit...   \n","5249  thanks for the reminder alex it s really good ...   \n","5250  health policy and systems research a methodolo...   \n","5251     start by heating your oil in a large fryin pan   \n","\n","                                                      1  \n","0     photo s by juha flinkman miia laine and sarita...  \n","1     ashburnham close is located within the ceremon...  \n","2     sorry about being less specific on here figure...  \n","3     how to decorate bathroom sets safe home inspir...  \n","4     osa scouting updated ratings potential contact...  \n","...                                                 ...  \n","5247  predictions about their upcoming performance f...  \n","5248  tuition december tuition will be sent home wit...  \n","5249  thanks for the reminder alex it s really good ...  \n","5250  health policy and systems research a methodolo...  \n","5251    start by heating your oil in a large frying pan  \n","\n","[5252 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-3b718606-bfdf-4784-85ad-c3f29dd03919\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>the photo s of juha flinkman miia laine and sa...</td>\n","      <td>photo s by juha flinkman miia laine and sarita...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ashburnham close is located within celebrated ...</td>\n","      <td>ashburnham close is located within the ceremon...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>sorry to be less specific on here figured most...</td>\n","      <td>sorry about being less specific on here figure...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>s how to decorate and bathroom sets safe home ...</td>\n","      <td>how to decorate bathroom sets safe home inspir...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>osa scouting updated rating potential contact ...</td>\n","      <td>osa scouting updated ratings potential contact...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5247</th>\n","      <td>prediction about its upcoming tournament perfo...</td>\n","      <td>predictions about their upcoming performance f...</td>\n","    </tr>\n","    <tr>\n","      <th>5248</th>\n","      <td>tuition december tuition will be send home wit...</td>\n","      <td>tuition december tuition will be sent home wit...</td>\n","    </tr>\n","    <tr>\n","      <th>5249</th>\n","      <td>thanks for the reminder alex it s really good ...</td>\n","      <td>thanks for the reminder alex it s really good ...</td>\n","    </tr>\n","    <tr>\n","      <th>5250</th>\n","      <td>health policy and systems research a methodolo...</td>\n","      <td>health policy and systems research a methodolo...</td>\n","    </tr>\n","    <tr>\n","      <th>5251</th>\n","      <td>start by heating your oil in a large fryin pan</td>\n","      <td>start by heating your oil in a large frying pan</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5252 rows × 2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3b718606-bfdf-4784-85ad-c3f29dd03919')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-3b718606-bfdf-4784-85ad-c3f29dd03919 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-3b718606-bfdf-4784-85ad-c3f29dd03919');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-65f208b3-3fa6-42b8-a3b1-dafd0e7fdb9e\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-65f208b3-3fa6-42b8-a3b1-dafd0e7fdb9e')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-65f208b3-3fa6-42b8-a3b1-dafd0e7fdb9e button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["# start trainig\n","train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r-dBGuHtMTW0","executionInfo":{"status":"ok","timestamp":1701361751417,"user_tz":-330,"elapsed":568295,"user":{"displayName":"Tejas Amritkar","userId":"17689469195552350370"}},"outputId":"6e806ed8-9fc7-4c71-8f78-36062e0d944c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0m 38s (- 9m 31s) (5 6%) 4.1135\n","1m 13s (- 8m 36s) (10 12%) 2.9560\n","1m 49s (- 7m 53s) (15 18%) 2.0953\n","2m 24s (- 7m 14s) (20 25%) 1.4361\n","3m 0s (- 6m 36s) (25 31%) 0.9666\n","3m 35s (- 5m 59s) (30 37%) 0.6461\n","4m 10s (- 5m 21s) (35 43%) 0.4404\n","4m 44s (- 4m 44s) (40 50%) 0.3054\n","5m 20s (- 4m 9s) (45 56%) 0.2218\n","5m 56s (- 3m 33s) (50 62%) 0.1637\n","6m 30s (- 2m 57s) (55 68%) 0.1251\n","7m 5s (- 2m 21s) (60 75%) 0.1000\n","7m 40s (- 1m 46s) (65 81%) 0.0781\n","8m 17s (- 1m 11s) (70 87%) 0.0651\n","8m 52s (- 0m 35s) (75 93%) 0.0546\n","9m 27s (- 0m 0s) (80 100%) 0.0495\n"]}]},{"cell_type":"code","source":["# Set dropout layers to eval mode\n","encoder.eval()\n","decoder.eval()\n","chrf_score = evaluateRandomly(encoder, decoder, test_pairs)"],"metadata":{"id":"ImQiNHbNKyPS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\" Example1: \"\"\"\n","example_1 = \"grammar: This sentences, has bads grammar and spelling!\"\n","\n","output_words, _ = evaluate(encoder, decoder, example_1, input_lang, output_lang)\n","prediction = ' '.join(output_words)\n","print(prediction)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"keErqilQfehQ","executionInfo":{"status":"ok","timestamp":1701369081774,"user_tz":-330,"elapsed":10,"user":{"displayName":"Tejas Amritkar","userId":"17689469195552350370"}},"outputId":"aebc2fff-cb48-42ff-c1b9-5bde26a8c77c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["grammar this sentences has bad grammar and spelling!\n"]}]},{"cell_type":"code","source":["\"\"\" Example2: \"\"\"\n","\n","example_2 = \"grammar: I am enjoys, writtings articles ons AI and I also enjoyed write articling on AI.\"\n","\n","output_words, _ = evaluate(encoder, decoder, example_2, input_lang, output_lang)\n","prediction = ' '.join(output_words)\n","print(prediction)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NVgAC480fgPf","executionInfo":{"status":"ok","timestamp":1701369148548,"user_tz":-330,"elapsed":22,"user":{"displayName":"Tejas Amritkar","userId":"17689469195552350370"}},"outputId":"6bf7e0bb-8bd2-4fa7-b08a-b9e2e1e4c793"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["grammar i am enjoy writtings articles one two AI and I also enjoy write articling on AI.\n"]}]},{"cell_type":"code","source":["# chrf score which is similarity matrixs\n","print(chrf_score)"],"metadata":{"id":"m8NTC6PaLke4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701361752501,"user_tz":-330,"elapsed":13,"user":{"displayName":"Tejas Amritkar","userId":"17689469195552350370"}},"outputId":"ccaecc3c-9a49-4f0d-ab7e-c657e75e4484"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["22.774357419700753\n"]}]}]}