{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNOyEFKBHvtBPHw8k/npv5m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Mount Google Drive to access the dataset\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uId7OVUypG4O","executionInfo":{"status":"ok","timestamp":1701279057660,"user_tz":-330,"elapsed":46977,"user":{"displayName":"Tejas Amritkar","userId":"17689469195552350370"}},"outputId":"7468ec02-b1bc-4beb-e0b9-fd66a5451649"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"id":"tGI8j_GtoLS0","executionInfo":{"status":"ok","timestamp":1701279111253,"user_tz":-330,"elapsed":4,"user":{"displayName":"Tejas Amritkar","userId":"17689469195552350370"}}},"outputs":[],"source":["from __future__ import unicode_literals, print_function, division\n","from io import open\n","import unicodedata\n","import re\n","import random\n","import csv\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","\n","import numpy as np\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","source":["SOS_token = 0\n","EOS_token = 1\n","\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n","        self.n_words = 2  # Count SOS and EOS\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1"],"metadata":{"id":"HicI8S8ooj-M","executionInfo":{"status":"ok","timestamp":1701279062299,"user_tz":-330,"elapsed":9,"user":{"displayName":"Tejas Amritkar","userId":"17689469195552350370"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Turn a Unicode string to plain ASCII, thanks to\n","# https://stackoverflow.com/a/518232/2809427\n","def unicodeToAscii(s):\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn'\n","    )\n","\n","# Lowercase, trim, and remove non-letter characters\n","def normalizeString(s):\n","    s = unicodeToAscii(s.lower().strip())\n","    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n","    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n","    return s.strip()"],"metadata":{"id":"NqALDtQypBEe","executionInfo":{"status":"ok","timestamp":1701279062299,"user_tz":-330,"elapsed":7,"user":{"displayName":"Tejas Amritkar","userId":"17689469195552350370"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def readLangs(lang1, lang2, reverse=False):\n","    print(\"Reading lines...\")\n","\n","    # Specify the path to your CSV file\n","    csv_file_path = '/content/drive/MyDrive/EE782_Grammer_Checker_Project/small_dataframe.csv'\n","\n","    # Read the CSV file and split into lines\n","    with open(csv_file_path, 'r', encoding='utf-8') as csvfile:\n","        reader = csv.reader(csvfile)\n","        lines = [row for row in reader]\n","\n","    # Normalize the strings\n","    pairs = [[normalizeString(row[0]), normalizeString(row[1])] for row in lines]\n","    input_lang = Lang(lang1)\n","    output_lang = Lang(lang2)\n","\n","    return input_lang, output_lang, pairs"],"metadata":{"id":"CkYpV8HEpCwp","executionInfo":{"status":"ok","timestamp":1701279251184,"user_tz":-330,"elapsed":4,"user":{"displayName":"Tejas Amritkar","userId":"17689469195552350370"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["MAX_LENGTH = 10\n","\n","eng_prefixes = (\n","    \"i am \", \"i m \",\n","    \"he is\", \"he s \",\n","    \"she is\", \"she s \",\n","    \"you are\", \"you re \",\n","    \"we are\", \"we re \",\n","    \"they are\", \"they re \"\n",")\n","\n","def filterPair(p):\n","    return len(p[0].split(' ')) < MAX_LENGTH and \\\n","        len(p[1].split(' ')) < MAX_LENGTH and \\\n","        p[1].startswith(eng_prefixes)\n","\n","\n","def filterPairs(pairs):\n","    return [pair for pair in pairs if filterPair(pair)]"],"metadata":{"id":"lPcLJTqrpR2O","executionInfo":{"status":"ok","timestamp":1701279251185,"user_tz":-330,"elapsed":3,"user":{"displayName":"Tejas Amritkar","userId":"17689469195552350370"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def prepareData(lang1, lang2, reverse=False):\n","    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n","    print(\"Read %s sentence pairs\" % len(pairs))\n","    pairs = filterPairs(pairs)\n","    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n","    print(\"Counting words...\")\n","    for pair in pairs:\n","        input_lang.addSentence(pair[0])\n","        output_lang.addSentence(pair[1])\n","    print(\"Counted words:\")\n","    print(input_lang.name, input_lang.n_words)\n","    print(output_lang.name, output_lang.n_words)\n","    return input_lang, output_lang, pairs\n","\n","input_lang, output_lang, pairs = prepareData('correct', 'incoorect', True)\n","print(random.choice(pairs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"32K-k3P4pSXX","executionInfo":{"status":"ok","timestamp":1701279282907,"user_tz":-330,"elapsed":710,"user":{"displayName":"Tejas Amritkar","userId":"17689469195552350370"}},"outputId":"3e12213a-d669-40ba-fe42-732b42cd04cd"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading lines...\n","Read 10001 sentence pairs\n","Trimmed to 8 sentence pairs\n","Counting words...\n","Counted words:\n","correct 53\n","incoorect 50\n","['i m gaining degree controlled in my life', 'i m gaining more control over my life']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Pg074mMHpVPp"},"execution_count":null,"outputs":[]}]}